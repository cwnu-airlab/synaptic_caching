## Synaptic Caching

Synaptic Caching은 대규모 언어 모델(LLM)의 효율성을 극대화하기 위한 기술로, 모델이 한 번 생각한 내용을 뇌(메모리)에 저장해두고, 똑같은 질문이나 문맥이 나오면 다시 생각하지 않고 바로 꺼내 쓰는 기술을 말합니다.
AI 모델도 입력을 처리할 때 내부의 수많은 파라미터(가중치)를 거치며 Activation States를 만들어냅니다. 
Synaptic Caching은 이 중간 연산 결과를 저장해 두는 것입니다. 마치 뇌가 이미 처리한 정보를 시냅스에 잠시 유지하는 것과 같습니다.


기존 방식과 비교하면 다음과 같습니다.
- 캐싱이 없을 때 (비효율적)
   - 사용자가 긴 문서를 주고 질문을 합니다.
   - 모델은 문서를 처음부터 끝까지 토큰화하고 연산합니다.
   - 사용자가 추가 질문을 합니다.
   - 모델은 아까 읽은 문서를 처음부터 다시 연산해야 합니다. (중복 연산 발생)
- Synaptic Caching (Prompt Caching) 적용 시
   - 사용자가 긴 문서를 줍니다.
   - 모델이 연산을 수행하면서, 각 토큰에 대한 Key-Value(KV) 상태값을 캐시(Cache) 메모리에 저장합니다.
   - 사용자가 추가 질문을 합니다.
   - 모델은 문서를 다시 읽지 않고, 저장된 캐시 데이터(KV Cache)를 그대로 가져와서 답변만 생성합니다.

KV Cache 관리(Attention Sink)를 위하여 다음 연구를 참조합니다.
- https://github.com/mit-han-lab/streaming-llm
